# Обучаемый Веб-Скрапер: Дизайн-Документ по Машинному Обучению

## Содержание

- [Введение](#введение)
    - [Описание Проблемы](#описание-проблемы)
    - [Цели и Задачи](#цели-и-задачи)
    - [Ограничения](#ограничения)
- [Описание Данных](#описание-данных)
    - [Источники Данных](#источники-данных)
    - [Форматы Данных](#форматы-данных)
- [Архитектура Модели](#архитектура-модели)
    - [Выбор Алгоритмов](#выбор-алгоритмов)
    - [Гиперпараметры](#гиперпараметры)
    - [Особенности Реализации](#особенности-реализации)
- [Инфраструктурные Требования](#инфраструктурные-требования)
    - [Системные Требования](#системные-требования)
    - [Хранение Данных](#хранение-данных)
- [Этапы Разработки](#этапы-разработки)
    - [Временные Рамки](#временные-рамки)
    - [Ресурсы и Ответственные Лица](#ресурсы-и-ответственные-лица)
- [Тестирование и Валидация](#тестирование-и-валидация)
    - [Метрики Качества](#метрики-качества)
    - [План Тестирования](#план-тестирования)
    - [План Валидации](#план-валидации)
- [Развертывание](#развертывание)
- [Идеи для Дальнейшего Развития](#идеи-для-дальнейшего-развития)
- [Словарь Терминов](#словарь-терминов)
- [Дополнительные Ресурсы](#дополнительные-ресурсы)

## Введение

### Описание Проблемы

Скрапинг данных с веб-сайтов часто требует обхода большого числа нерелевантных страниц, что тратит время и ресурсы. Необходим инструмент, который мог бы "умно" выбирать, какие страницы обходить для сбора нужной информации..
Видео с наглядным описанием проблемы:
https://www.youtube.com/watch?v=05xoR--oJSI

### Цели и Задачи

1. Разработать обучаемый веб-скрапер на Selenium.
2. Сделать его применимым к различным типам сайтов.
3. Обеспечить адаптивность к изменениям в структуре сайтов.

### Ограничения

- Не использовать URL для обучения, так как они не всегда присутствуют на целевых страницах.
- Соблюдение законов и правил, касающихся веб-скрапинга.

## Описание Данных

Первичные данные можно загрузить по следующей ссылке 
https://drive.google.com/drive/folders/16ErrDDfLu2Vw7rhK6ioqzsX5TqjH3lcM

Загруженные страницы хранятся в отдельных файлах в структуре папок. 
В таблице pages.csv хранятся данные о том, как были загружены эти страницы.

Поля pages.csv:    

url_id   - идентификатор url
bank_i - код банка
url - собственно url, у нее может быть специфичный формат если используется xpath
parent_url   - родительская url с которой заходили на эту url

Ссылки могут содержать команды типа <>xclick://*[contains(@class,"f0cgs")]
То есть после открытия url кликнуть такой-то элемент по xpath указанному после <>xclick:

Cтруктура папок в каталоге pages:

название папки соответствует коду банка.
имена файлов в папках это url_id

Таким образом, в первичных данных сохранены все страницы начиная с корня сайта, информация как переходили на следующу. страницу и сам код страниц.
Из этих данных нужно сформировать обучающий набор.
В настоящий момент сохранены пути, и страницы двух банков ведущие к страницам с информацией об их отделениях.

### Источники Данных


1. Примеры сайтов для начального обучения (например, сайты нескольких крупных банков).
2. XPath и контекст (текст, заголовки и другие атрибуты) элементов на веб-страницах.

### Форматы Данных

- XPath для идентификации элементов.
- HTML для структуры страницы.
- JSON или XML для хранения конфигураций и данных обученной модели.

## Архитектура Модели

### Выбор Алгоритмов

1. Обучение с подкреплением для оптимизации стратегии обхода страниц.
2. Случайный лес или градиентный бустинг для классификации страниц по релевантности.

### Гиперпараметры

- Скорость обучения, гамма и другие параметры для обучения с подкреплением.
- Количество деревьев и глубина для алгоритмов классификации.

### Особенности Реализации

1. Модуль для сбора данных через Selenium.
2. Модуль для обработки и хранения данных.
3. Модуль для обучения и применения ML-моделей.
4. Возможность плагинного расширения для поддержки новых типов сайтов.

## Инфраструктурные Требования

### Системные Требования

- ОС: Linux/Windows/Mac
- Python 3.x
- Доступ в интернет

### Хранение Данных

- Локальное хранилище или облачные решения типа AWS S3 для хранения моделей и логов.

## Этапы Разработки

1. **Исследование и анализ** (3 недели)
    - Анализ типичных структур сайтов.
    - Выбор примеров для обучения.
2. **Разработка MVP** (4 недели)
    - Прототип сбора данных.
    - Простая модель для классификации страниц.

## Заключение

В этом документе описаны основные аспекты разработки обучаемого веб-скрапера. Планируется выполнение проекта в несколько этапов с использованием методологии агильной разработки.

- Локальное хранилище или облачные решения типа AWS S3 для хранения моделей и логов.

## Этапы разработки и план

1. **Исследование и анализ** (2 недели)
    - Анализ типичных структур сайтов.
    - Выбор примеров для обучения.
2. **Разработка MVP** (3 недели)
    - Прототип сбора данных.
    - Простая модель для классификации страниц.
3. **Тестирование и итерация** (2 недели)
    - Unit-тесты и интеграционные тесты.
    - Итерация на основе результатов.
4. **Рефакторинг и оптимизация** (1 неделя)
    - Оптимизация кода и модели.
5. **Документация и развертывание** (1 неделя)
    - Написание документации и инструкций.
    - Подготовка к развертыванию.

### Ресурсы и ответственные лица

- Data Scientist: Моделирование и алгоритмы.
- Backend Developer: Реализация и интеграция.
- DevOps: Развертывание и поддержка.

## Тестирование и валидация

### Метрики качества

- Точность (Precision) и полнота (Recall) в выборе релевантных страниц.
- Скорость выполнения.

### План тестирования

- Набор unit-тестов для каждого модуля.
- Интеграционные тесты для проверки взаимодействия модулей.

### План валидации

- Кросс-валидация на разных доменах и структурах сайтов.
- Стресс-тесты на больших сайтах.

## Развертывание и поддержка

- Инструкции для развертывания через Docker.
- План по мониторингу и обновлению моделей.

## Дополнительные идеи и улучшения

- Интеграция с системами аналитики данных.
- Создание пользовательского интерфейса для настройки и мониторинга скрапера.

## Словарь терминов

- Скрапинг: Процесс автоматического сбора данных с веб-сайтов.
- Reinforcement Learning: Обучение с подкреплением.

## Дополнительные материалы

- Документация Selenium
- Обзоры алгоритмов машинного обучения
